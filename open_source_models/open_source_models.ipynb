{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "from transformers.utils import logging\n",
    "# from sentence_transformers import util, SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import gradio\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversational Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(task=\"conversational\",\n",
    "                   model=\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "user_message = \"\"\"\n",
    "Define PI\n",
    "\"\"\"\n",
    "\n",
    "conversation = Conversation(user_message)\n",
    "conversation_response = chatbot(conversation)\n",
    "print(conversation_response)\n",
    "\n",
    "conversation.add_message({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "Define square root?\n",
    "\"\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chatbot, user_message, conversation, conversation_response\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transational and Summarization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transalator = pipeline(task=\"translation\",\n",
    "                       model=\"facebook/nllb-200-distilled-600M\",\n",
    "                       torch_dtype=torch.bfloat16)\n",
    "\n",
    "text = \"\"\"\n",
    "We are the children of planet earth.\\\n",
    "We are the most intelligent species on earth.\n",
    "\"\"\"\n",
    "\n",
    "translated_text = transalator(text,\n",
    "                              src_lang='eng_Latin',\n",
    "                              tgt_lang='hin_Deva',)\n",
    "\n",
    "print(translated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transalator, translated_text, text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(task=\"summarization\",\n",
    "                       model=\"facebook/bart-large-cnn\",\n",
    "                       torch_dtype=torch.bfloat16)\n",
    "\n",
    "text = \"\"\"Paris is the capital and most populous city of France, with\n",
    "          an estimated population of 2,175,601 residents as of 2018,\n",
    "          in an area of more than 105 square kilometres (41 square\n",
    "          miles). The City of Paris is the centre and seat of\n",
    "          government of the region and province of ÃŽle-de-France, or\n",
    "          Paris Region, which has an estimated population of\n",
    "          12,174,880, or about 18 percent of the population of France\n",
    "          as of 2017.\"\"\"\n",
    "\n",
    "summarized_text = summarizer(text,\n",
    "                              min_length=10,\n",
    "                              max_length=100)\n",
    "\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del summarizer, translated_text, text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences1 = ['The cat sits outside',\n",
    "              'A man is playing guitar',\n",
    "              'The movies are awesome']\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "print(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, sentences1, sentences2, embeddings1, embeddings1, cosine_scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ashraq/esc50\", split=\"train[0:10]\")\n",
    "# dataset = load_from_disk(\"./models/ashraq/esc50/train\")\n",
    "\n",
    "audio_sample = dataset[0]\n",
    "IPythonAudio(audio_sample[\"audio\"][\"array\"], rate=audio_sample[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "zero_shot_classifier = pipeline(task=\"zero-shot-audio-classification\",\n",
    "                                model=\"laion/clap-htsat-unfused\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=48_000))\n",
    "\n",
    "audio_sample = dataset[0]\n",
    "\n",
    "candidate_labels = [\n",
    "    \"sound of dog\",\n",
    "    \"sound of vacuum cleaner\"\n",
    "]\n",
    "\n",
    "zero_shot_classifier(audio_sample[\"audio\"][\"array\"], candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, audio_sample, zero_shot_classifier, candidate_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"librispeech_asr\", split=\"train.clean.100\",\n",
    "                       streaming=True, trsut_remote_code=True)\n",
    "\n",
    "example = next(iter(dataset))\n",
    "IPythonAudio(example[\"audio\"][\"array\"],\n",
    "             rate=example[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "               model=\"distil-whisper/distil-small.en\")\n",
    "\n",
    "demo = gradio.Blocks()\n",
    "\n",
    "\n",
    "def transcribe_speech(filepath):\n",
    "    if (filepath is None):\n",
    "        gradio.warning(\"No audio found, please try again!\")\n",
    "        return \"\"\n",
    "    audio, sampling_rate = soundfile.read(filepath)\n",
    "    audio_transposed = np.transpose(audio)\n",
    "    audio_mono = librosa.to_mono(audio_transposed)\n",
    "    audio_resampled = librosa.resample(audio_mono,\n",
    "                                       orig_sr=sampling_rate,\n",
    "                                       target_sr=16000)\n",
    "    output = asr(audio_resampled,\n",
    "                 max_new_tokens=256,\n",
    "                 chunk_length_s=30,\n",
    "                 batch_size=8)\n",
    "    return output[\"text\"]\n",
    "\n",
    "mic_transcribe = gradio.Interface(fn=transcribe_speech,\n",
    "                                  inputs=gradio.Audio(sources=\"microphone\",\n",
    "                                                      type=\"filepath\",),\n",
    "                                  outputs=gradio.Textbox(label=\"Transcription\",\n",
    "                                                         lines=3),\n",
    "                                  allow_flaggin=\"never\")\n",
    "\n",
    "file_transcribe = gradio.Interface(fn=transcribe_speech,\n",
    "                                   inputs=gradio.Audio(sources=\"upload\",\n",
    "                                                       type=\"filepath\"),\n",
    "                                    outputs=gradio.Textbox(label=\"Transcription\",\n",
    "                                                           lines=3),\n",
    "                                    allow_flagging=\"never\")\n",
    "\n",
    "with demo:\n",
    "    gradio.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, example, asr, demo, mic_transcribe, file_transcribe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrator = pipeline(task=\"text-to-speech\",\n",
    "                    model=\"kakao-enterprise/vits-ljs\")\n",
    "\n",
    "text = \"\"\"\n",
    "Researchers at the Allen Institute for AI, \\\n",
    "HuggingFace, Microsoft, the University of Washington, \\\n",
    "Carnegie Mellon University, and the Hebrew University of \\\n",
    "Jerusalem developed a tool that measures atmospheric \\\n",
    "carbon emitted by cloud servers while training machine \\\n",
    "learning models. After a modelâ€™s size, the biggest variables \\\n",
    "were the serverâ€™s location and time of day it was active.\n",
    "\"\"\"\n",
    "\n",
    "narrated_text = narrator(text)\n",
    "\n",
    "IPythonAudio(narrated_text[\"audio\"][0], rate=narrated_text[\"sampling_rate\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
