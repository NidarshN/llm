{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import gradio\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "from transformers import pipeline, Conversation, SamModel, SamProcessor, BlipForImageTextRetrieval, AutoProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering, CLIPModel\n",
    "from transformers.utils import logging\n",
    "# from sentence_transformers import util, SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "from helper import load_image_from_url, render_results_in_image, ignore_warnings, summarize_predictions_natural_language, show_pipe_masks_on_image\n",
    "logging.set_verbosity_error()\n",
    "ignore_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversational Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(task=\"conversational\",\n",
    "                   model=\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "user_message = \"\"\"\n",
    "Define PI\n",
    "\"\"\"\n",
    "\n",
    "conversation = Conversation(user_message)\n",
    "conversation_response = chatbot(conversation)\n",
    "print(conversation_response)\n",
    "\n",
    "conversation.add_message({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "Define square root?\n",
    "\"\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chatbot, user_message, conversation, conversation_response\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transational and Summarization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transalator = pipeline(task=\"translation\",\n",
    "                       model=\"facebook/nllb-200-distilled-600M\",\n",
    "                       torch_dtype=torch.bfloat16)\n",
    "\n",
    "text = \"\"\"\n",
    "We are the children of planet earth.\\\n",
    "We are the most intelligent species on earth.\n",
    "\"\"\"\n",
    "\n",
    "translated_text = transalator(text,\n",
    "                              src_lang='eng_Latin',\n",
    "                              tgt_lang='hin_Deva',)\n",
    "\n",
    "print(translated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transalator, translated_text, text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(task=\"summarization\",\n",
    "                       model=\"facebook/bart-large-cnn\",\n",
    "                       torch_dtype=torch.bfloat16)\n",
    "\n",
    "text = \"\"\"Paris is the capital and most populous city of France, with\n",
    "          an estimated population of 2,175,601 residents as of 2018,\n",
    "          in an area of more than 105 square kilometres (41 square\n",
    "          miles). The City of Paris is the centre and seat of\n",
    "          government of the region and province of ÃŽle-de-France, or\n",
    "          Paris Region, which has an estimated population of\n",
    "          12,174,880, or about 18 percent of the population of France\n",
    "          as of 2017.\"\"\"\n",
    "\n",
    "summarized_text = summarizer(text,\n",
    "                              min_length=10,\n",
    "                              max_length=100)\n",
    "\n",
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del summarizer, translated_text, text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences1 = ['The cat sits outside',\n",
    "              'A man is playing guitar',\n",
    "              'The movies are awesome']\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "print(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, sentences1, sentences2, embeddings1, embeddings1, cosine_scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ashraq/esc50\", split=\"train[0:10]\")\n",
    "# dataset = load_from_disk(\"./models/ashraq/esc50/train\")\n",
    "\n",
    "audio_sample = dataset[0]\n",
    "IPythonAudio(audio_sample[\"audio\"][\"array\"], rate=audio_sample[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "zero_shot_classifier = pipeline(task=\"zero-shot-audio-classification\",\n",
    "                                model=\"laion/clap-htsat-unfused\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=48_000))\n",
    "\n",
    "audio_sample = dataset[0]\n",
    "\n",
    "candidate_labels = [\n",
    "    \"sound of dog\",\n",
    "    \"sound of vacuum cleaner\"\n",
    "]\n",
    "\n",
    "zero_shot_classifier(audio_sample[\"audio\"][\"array\"], candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, audio_sample, zero_shot_classifier, candidate_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"librispeech_asr\", split=\"train.clean.100\",\n",
    "                       streaming=True, trsut_remote_code=True)\n",
    "\n",
    "example = next(iter(dataset))\n",
    "IPythonAudio(example[\"audio\"][\"array\"],\n",
    "             rate=example[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "               model=\"distil-whisper/distil-small.en\")\n",
    "\n",
    "demo = gradio.Blocks()\n",
    "\n",
    "\n",
    "def transcribe_speech(filepath):\n",
    "    if (filepath is None):\n",
    "        gradio.warning(\"No audio found, please try again!\")\n",
    "        return \"\"\n",
    "    audio, sampling_rate = soundfile.read(filepath)\n",
    "    audio_transposed = np.transpose(audio)\n",
    "    audio_mono = librosa.to_mono(audio_transposed)\n",
    "    audio_resampled = librosa.resample(audio_mono,\n",
    "                                       orig_sr=sampling_rate,\n",
    "                                       target_sr=16000)\n",
    "    output = asr(audio_resampled,\n",
    "                 max_new_tokens=256,\n",
    "                 chunk_length_s=30,\n",
    "                 batch_size=8)\n",
    "    return output[\"text\"]\n",
    "\n",
    "mic_transcribe = gradio.Interface(fn=transcribe_speech,\n",
    "                                  inputs=gradio.Audio(sources=\"microphone\",\n",
    "                                                      type=\"filepath\",),\n",
    "                                  outputs=gradio.Textbox(label=\"Transcription\",\n",
    "                                                         lines=3),\n",
    "                                  allow_flaggin=\"never\")\n",
    "\n",
    "file_transcribe = gradio.Interface(fn=transcribe_speech,\n",
    "                                   inputs=gradio.Audio(sources=\"upload\",\n",
    "                                                       type=\"filepath\"),\n",
    "                                    outputs=gradio.Textbox(label=\"Transcription\",\n",
    "                                                           lines=3),\n",
    "                                    allow_flagging=\"never\")\n",
    "\n",
    "with demo:\n",
    "    gradio.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, example, asr, demo, mic_transcribe, file_transcribe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrator = pipeline(task=\"text-to-speech\",\n",
    "                    model=\"kakao-enterprise/vits-ljs\")\n",
    "\n",
    "text = \"\"\"\n",
    "Researchers at the Allen Institute for AI, \\\n",
    "HuggingFace, Microsoft, the University of Washington, \\\n",
    "Carnegie Mellon University, and the Hebrew University of \\\n",
    "Jerusalem developed a tool that measures atmospheric \\\n",
    "carbon emitted by cloud servers while training machine \\\n",
    "learning models. After a modelâ€™s size, the biggest variables \\\n",
    "were the serverâ€™s location and time of day it was active.\n",
    "\"\"\"\n",
    "\n",
    "narrated_text = narrator(text)\n",
    "\n",
    "IPythonAudio(narrated_text[\"audio\"][0], rate=narrated_text[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = pipeline(task=\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "filepath = \"\"\n",
    "raw_img = Image.open(filepath)\n",
    "raw_img.resize((569, 491))\n",
    "pipeline_output = object_detector(raw_img)\n",
    "processed_img = render_results_in_image(raw_img, pipeline_output)\n",
    "\n",
    "def get_pipeline_prediction(pil_img):\n",
    "    pipeline_output = object_detector(pil_img)\n",
    "    # text = summarize_predictions_natural_language(pipeline_output)\n",
    "    processed_img = render_results_in_image(pil_img, pipeline_output)\n",
    "    return processed_img, text\n",
    "\n",
    "demo = gradio.Interface(fn=get_pipeline_prediction,\n",
    "                        inputs=gradio.Image(label=\"Input Image\",\n",
    "                            type=\"pil\"),\n",
    "                        outputs=gradio.Image(label=\"Output Image with predicted instances\", type=\"pil\"),\n",
    ")\n",
    "\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del object_detector, filepath, raw_img, pipeline_output, processed_img, demo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_pipeline = pipeline(task=\"mask-generation\", model=\"Zigeng/SlimSAM-uniform-77\")\n",
    "filepath = \"\"\n",
    "raw_image = Image.open(filepath)\n",
    "raw_image.resize((720, 375))\n",
    "output = sam_pipeline(raw_image, points_per_batch=32)\n",
    "show_pipe_masks_on_image(raw_image, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_estimator = pipeline(task=\"depth-estimation\",\n",
    "                           model=\"Intel/dpt-hybrid-midas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(input_image):\n",
    "    output = depth_estimator(input_image)\n",
    "\n",
    "    prediction = torch.nn.fucntional.interpolate(output[\"predicted_depth\"].unsqueeze(1),\n",
    "                                                 size=input_image.size[::-1],\n",
    "                                                 mode=\"bicubic\",\n",
    "                                                 align_corners=False)\n",
    "    \n",
    "    output = prediction.squeeze().numpy()\n",
    "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    depth = Image.fromarray(formatted)\n",
    "    return depth\n",
    "\n",
    "iface = gradio.Interface(fn=launch,\n",
    "                         inputs=gradio.Image(type=\"pil\"),\n",
    "                         outputs=gradio.Image(type=\"pil\"))\n",
    "iface.launch(share=True, server_port=int(os.environ[\"PORT1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sam_pipline, filepath, raw_image, output, iface\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "text = \"an image of a woman and a dog on the beach\"\n",
    "\n",
    "inputs = processor(images=raw_image,\n",
    "                   text=text,\n",
    "                   return_tensors='pt')\n",
    "itm_scores = model(**inputs)[0]\n",
    "itm_scores = torch.nn.functional.softmax(itm_scores, dim=1)\n",
    "\n",
    "print(f\"The image and text match score is: {itm_scores[0][1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_text_model, processor, text, inputs, itm_scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cap_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "text = \"a photograph of\"\n",
    "inputs = processor(raw_img, text, return_tensors=\"pt\")\n",
    "output= model.generate(**inputs)\n",
    "print(f\"Conditional Captioning: {processor.decode(output[0], skip_special_tokens=True)}\")\n",
    "\n",
    "inputs = processor(raw_img, return_tensors=\"pt\")\n",
    "output= model.generate(**inputs)\n",
    "print(f\"Unconditional Captioning: {processor.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_cap_model, processor, text, inputs, output\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "question = \"How many dogs are in the picture?\"\n",
    "\n",
    "inputs = processor(raw_img, question, return_tensors=\"pt\")\n",
    "output= model.generate(**inputs)\n",
    "\n",
    "print(f\"Model Answer: {processor.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, processor, question, inputs, output\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "labels = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "inputs = processor(text=labels,\n",
    "                   images=raw_img,\n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True)\n",
    "output = model(**inputs)\n",
    "probs = list(output.logits_per_image.softmax(dim=1)[0])\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  print(f\"label: {labels[i]} - probability of {probs[i].item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
